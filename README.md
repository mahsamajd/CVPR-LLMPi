# CVPR-LLMPi

**CVPR-LLMPi** explores the deployment of Large Language Models (LLMs) on edge devices like the Raspberry Pi 5 through efficient post-training quantization techniques (Q2, Q4, Q6, Q8).  
The project focuses on enabling real-time inference while optimizing energy efficiency and maintaining model accuracy.

---

## ğŸ“š Project Overview
- Trade-off analysis between model accuracy, latency, and energy efficiency with quantization for edge AI applications
- Post-training quantization (PTQ) of Large Language Models (LLMs) including Phi-3, Gemma, and Llama-3 across multiple bit-widths (Q2, Q4, Q6, Q8)
- Quantization-Aware Training (QAT) applied to BitNet models (ternary quantization, Q1.58)
- Benchmarking model performance using Tokens per Second (TPS), Tokens per Joule (TPJ), and Words per Battery Life (W/BL)
- Evaluation of quantization impact on semantic coherence using NUBIA scores
- Real-world deployment and energy measurements on Raspberry Pi 5.

---

## ğŸ“ˆ Accuracy vs. Latency Trade-off
![Accuracy vs Latency Plot](Figures/Accuracy_over_Latency/nubia.png)
This work presents a detailed evaluation of how quantization impacts the trade-off between accuracy and latency in LLM inference on edge devices.
- Models such as **Phi3B** and **BitNet** exhibit strong resilience to quantization, maintaining high NUBIA scores even at lower bit-widths (Q4, Q6).
- **BitNet models** using quantization-aware training (Q1.58) achieve real-time inference speeds with minimal loss in semantic quality.
- Models like **Llama1B**, **Gemma2B**, and **Phi3B** show that PTQ can serve as a viable and efficient alternative to QAT, offering competitive accuracy-latency trade-offs.

---

## ğŸ¯ Objectives
- Enable real-time Large Language Model (LLM) inference on low-power embedded devices
- Reduce energy consumption and improve throughput through post-training quantization (PTQ) and quantization-aware training (QAT)
- Benchmark trade-offs between model precision, latency, and semantic accuracy
- Demonstrate practical deployment of quantized LLMs (Phi-3, Gemma, Llama-3, BitNet) on Raspberry Pi 5
- Evaluate performance using TPS (Tokens Per Second), TPJ (Tokens Per Joule), W/BL (Words Per Battery Life), and NUBIA scores

---

## âš™ï¸ Requirements
- Python 3.10+
- Llama.cpp
- Raspberry Pi 5 (or compatible ARM64 device)

---

## ğŸ“ Repository Structure
```text
.
â”œâ”€â”€ Code
â”‚   â”œâ”€â”€ Nubia_Score
â”‚   â”‚   â”œâ”€â”€ llm_response_bitnet.py
â”‚   â”‚   â”œâ”€â”€ llm_response.py
â”‚   â”‚   â””â”€â”€ nubia.py
â”‚   â””â”€â”€ Throughput
â”‚       â”œâ”€â”€ Throughput_BitNet.py
â”‚       â””â”€â”€ Throughput.py
â”œâ”€â”€ Figures
â”‚   â”œâ”€â”€ Accuracy_over_Latency
â”‚   â”‚   â””â”€â”€ nubia.png
â”‚   â”œâ”€â”€ LLM_Response_Comparison
â”‚   â”‚   â””â”€â”€ LLM_Response_Comparison.pdf
â”‚   â”œâ”€â”€ Nubia_Score
â”‚   â”œâ”€â”€ TPJ
â”‚   â”‚   â””â”€â”€ TPJ.pdf
â”‚   â”œâ”€â”€ TPS
â”‚   â”‚   â””â”€â”€ Final_TPS.pdf
â”‚   â””â”€â”€ WPBL
â”‚       â””â”€â”€ WPBL.pdf
â”œâ”€â”€ LLM_Responses
â”‚   â”œâ”€â”€ llm_response_without_whisper_Bitnet_results_CVPR_contextbased
â”‚   â”‚   â”œâ”€â”€ bitnet_b1_58_large
â”‚   â”‚   â”‚   â””â”€â”€ bitnet_b1_58_large.txt
â”‚   â”‚   â””â”€â”€ llama3_8B
â”‚   â”‚       â””â”€â”€ llama3_8B.txt
â”‚   â””â”€â”€ llm_response_without_whisper_results_CVPR_contextbased
â”‚      
â””â”€â”€ README.md


